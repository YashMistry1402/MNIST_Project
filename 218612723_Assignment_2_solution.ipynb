{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "218612723_Assignment_2_solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjif_5FsdOFz"
      },
      "source": [
        "# SIT744 Assignment 2: Efficient Training of Convolutional Neural Network \n",
        "\n",
        "<div class=\"alert-info\">\n",
        "    <p>Due: <strong>11:59pm 21 September 2020</strong>  (Monday)</p>\n",
        "\n",
        "This is an <strong>individual</strong> assignment. It contributes <strong>45%</strong> to your final mark. Read the assignment instruction carefully.\n",
        "\n",
        "<h2> What to submit </h2>\n",
        "\n",
        "<p>\n",
        "This assignment is to be completed individually and submitted to CloudDeakin. <strong>By the due date, you are required to submit the following files to the corresponding Assignment (Dropbox) in CloudDeakin</strong>:\n",
        "\n",
        "<ol>\n",
        "<li>\t<strong>[YourID]_assignment2_solution.ipynp</strong>:  This is your Python notebook solution source file. </li>\n",
        "<li>\t<strong>[YourID]_assingment2_output.html</strong>: This is the output of your Python notebook solution <emph>exported</emph> in HTML format.</li>\n",
        "<li>\tExtra files needed to complete your assignment, if any (e.g., images used in your answers).</li>\n",
        "</ol>\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "For example, if your student ID is: 123456, you will then need to submit the following files:\n",
        "<ul>\n",
        "<li> 123456_assignment2_solution.ipynp </li>\n",
        "<li> 123456_assignment2_output.html</li>\n",
        "</ul>\n",
        "</p>\n",
        "\n",
        "<h2> Warning </h2>\n",
        "\n",
        "Some components of this assignment may involve heavy computation that runs for a long duration. Please start early to avoid missing the assignment due date.\n",
        "\n",
        "<h2> Marking criteria </h2>\n",
        "\n",
        "<p>\n",
        "Your submission will be marked using the following criteria.\n",
        "\n",
        "<ul>\n",
        "<li> Showing good effort through completed tasks.</li>\n",
        "<li> Applying deep learning theory to design suitable deep learning solutions for the tasks.</li>\n",
        "<li> Critically evaluating and reflecting on the pros and cons of various design decisions.</li>\n",
        "<li> Demonstrating creativity and resourcefulness in providing unique individual solutions.</li>\n",
        "<li> Showing attention to details through a good quality assignment report.</li>\n",
        "</ul>\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "Indicative weights of various tasks are provided, but the assignment will be marked by the overall quality per the above criteria.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twFQbltnm8da"
      },
      "source": [
        "## Assignment objective\n",
        "\n",
        "This assignment is to feedback on your learning in deep learning theory and its application to  data analytics or artificial intelligence problems.  \n",
        "\n",
        "It builds on Assignment 1 but requires a higher level of mastery of deep learning theory and programming/engineering skills. In particular, you will experience training a much deeper network on a large-scale dataset. You will encounter  practical issues that help you consolidate textbook learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ITc1hw_o7qV"
      },
      "source": [
        "## Task 1 Solving Fashion-MNIST with Convolutional Neural Networks\n",
        "\n",
        "*(weight ~18%)*\n",
        "\n",
        "In Assignment 1, you tackled the image classification problem in Fashion-MNIST. There, you used a Densely Connected Neural Network. You should now know that is not an optimal model architecture for the problem. In Assignment 2, you will apply the best practices of deep-learning computer vision to improve the image classification performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdHwmgwOpEfx"
      },
      "source": [
        "### Task 1.1 Revisit Fashion-MNIST classification with DNN\n",
        "\n",
        "*(weight ~2%)*\n",
        "\n",
        "Review your Assignment 1 solution, and reproduce the experiment here. Try to improve the model without changing the model architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p4g2EAVx72F",
        "outputId": "8acb4a60-e74e-4d81-c270-6de6d1ed9e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        }
      },
      "source": [
        "!pip install tensorflow\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the fashion-mnist pre-shuffled train data and test data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (50.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW9WZljqyDsY",
        "outputId": "f668f946-3117-4432-df8d-5d265cdc6268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#we rescale the model for smoothness \n",
        "x_train = x_train.astype('float32')/255\n",
        "x_test = x_test.astype('float32')/255\n",
        "\n",
        "#Breaking training data into training(50000) and validation(10000) sets\n",
        "(x_train, x_valid) = x_train[10000:], x_train[:10000]\n",
        "(y_train, y_valid) = y_train[10000:], y_train[:10000]\n",
        "\n",
        "# Reshaping input data from (28, 28) to (28, 28, 1)\n",
        "w, h = 28, 28\n",
        "x_train = x_train.reshape(x_train.shape[0], w, h, 1)\n",
        "x_valid = x_valid.reshape(x_valid.shape[0], w, h, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], w, h, 1)\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# we will define the shape of the input in the first layer of the neural network\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# taking a look at the summary of our model\n",
        "\n",
        "tf.keras.utils.plot_model(\n",
        "    model, to_file='model.png', show_shapes=False, show_layer_names=True,\n",
        "    rankdir='TB', expand_nested=False, dpi=96\n",
        ")\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
        "\n",
        "model.fit(x_train,\n",
        "         y_train,\n",
        "         batch_size=64,\n",
        "         epochs=75,\n",
        "         validation_data=(x_valid, y_valid),\n",
        "         callbacks=[checkpointer])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/75\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.6250 - accuracy: 0.7689\n",
            "Epoch 00001: val_loss improved from inf to 0.41385, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.6250 - accuracy: 0.7689 - val_loss: 0.4138 - val_accuracy: 0.8456\n",
            "Epoch 2/75\n",
            "775/782 [============================>.] - ETA: 0s - loss: 0.4331 - accuracy: 0.8417\n",
            "Epoch 00002: val_loss improved from 0.41385 to 0.34885, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.4333 - accuracy: 0.8415 - val_loss: 0.3489 - val_accuracy: 0.8757\n",
            "Epoch 3/75\n",
            "776/782 [============================>.] - ETA: 0s - loss: 0.3847 - accuracy: 0.8593\n",
            "Epoch 00003: val_loss improved from 0.34885 to 0.31546, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.3845 - accuracy: 0.8595 - val_loss: 0.3155 - val_accuracy: 0.8849\n",
            "Epoch 4/75\n",
            "776/782 [============================>.] - ETA: 0s - loss: 0.3604 - accuracy: 0.8672\n",
            "Epoch 00004: val_loss improved from 0.31546 to 0.29413, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.3599 - accuracy: 0.8674 - val_loss: 0.2941 - val_accuracy: 0.8907\n",
            "Epoch 5/75\n",
            "772/782 [============================>.] - ETA: 0s - loss: 0.3403 - accuracy: 0.8748\n",
            "Epoch 00005: val_loss improved from 0.29413 to 0.27786, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.3404 - accuracy: 0.8747 - val_loss: 0.2779 - val_accuracy: 0.8970\n",
            "Epoch 6/75\n",
            "776/782 [============================>.] - ETA: 0s - loss: 0.3240 - accuracy: 0.8810\n",
            "Epoch 00006: val_loss improved from 0.27786 to 0.26458, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.3242 - accuracy: 0.8809 - val_loss: 0.2646 - val_accuracy: 0.9013\n",
            "Epoch 7/75\n",
            "778/782 [============================>.] - ETA: 0s - loss: 0.3126 - accuracy: 0.8853\n",
            "Epoch 00007: val_loss improved from 0.26458 to 0.25713, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.3124 - accuracy: 0.8853 - val_loss: 0.2571 - val_accuracy: 0.9059\n",
            "Epoch 8/75\n",
            "780/782 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8882\n",
            "Epoch 00008: val_loss did not improve from 0.25713\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.3007 - accuracy: 0.8882 - val_loss: 0.2582 - val_accuracy: 0.9032\n",
            "Epoch 9/75\n",
            "774/782 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.8920\n",
            "Epoch 00009: val_loss improved from 0.25713 to 0.23940, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2912 - accuracy: 0.8922 - val_loss: 0.2394 - val_accuracy: 0.9098\n",
            "Epoch 10/75\n",
            "776/782 [============================>.] - ETA: 0s - loss: 0.2806 - accuracy: 0.8962\n",
            "Epoch 00010: val_loss improved from 0.23940 to 0.23714, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2803 - accuracy: 0.8963 - val_loss: 0.2371 - val_accuracy: 0.9122\n",
            "Epoch 11/75\n",
            "781/782 [============================>.] - ETA: 0s - loss: 0.2759 - accuracy: 0.8971\n",
            "Epoch 00011: val_loss improved from 0.23714 to 0.23126, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.2759 - accuracy: 0.8971 - val_loss: 0.2313 - val_accuracy: 0.9122\n",
            "Epoch 12/75\n",
            "779/782 [============================>.] - ETA: 0s - loss: 0.2686 - accuracy: 0.9017\n",
            "Epoch 00012: val_loss did not improve from 0.23126\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.2686 - accuracy: 0.9017 - val_loss: 0.2334 - val_accuracy: 0.9098\n",
            "Epoch 13/75\n",
            "769/782 [============================>.] - ETA: 0s - loss: 0.2648 - accuracy: 0.9020\n",
            "Epoch 00013: val_loss improved from 0.23126 to 0.22928, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.2648 - accuracy: 0.9019 - val_loss: 0.2293 - val_accuracy: 0.9155\n",
            "Epoch 14/75\n",
            "773/782 [============================>.] - ETA: 0s - loss: 0.2573 - accuracy: 0.9044\n",
            "Epoch 00014: val_loss improved from 0.22928 to 0.22628, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2572 - accuracy: 0.9043 - val_loss: 0.2263 - val_accuracy: 0.9142\n",
            "Epoch 15/75\n",
            "779/782 [============================>.] - ETA: 0s - loss: 0.2513 - accuracy: 0.9071\n",
            "Epoch 00015: val_loss improved from 0.22628 to 0.21799, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2510 - accuracy: 0.9073 - val_loss: 0.2180 - val_accuracy: 0.9174\n",
            "Epoch 16/75\n",
            "779/782 [============================>.] - ETA: 0s - loss: 0.2482 - accuracy: 0.9094\n",
            "Epoch 00016: val_loss did not improve from 0.21799\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.2483 - accuracy: 0.9094 - val_loss: 0.2181 - val_accuracy: 0.9218\n",
            "Epoch 17/75\n",
            "779/782 [============================>.] - ETA: 0s - loss: 0.2448 - accuracy: 0.9095\n",
            "Epoch 00017: val_loss improved from 0.21799 to 0.21002, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2448 - accuracy: 0.9095 - val_loss: 0.2100 - val_accuracy: 0.9212\n",
            "Epoch 18/75\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.2380 - accuracy: 0.9115\n",
            "Epoch 00018: val_loss did not improve from 0.21002\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.2380 - accuracy: 0.9115 - val_loss: 0.2236 - val_accuracy: 0.9158\n",
            "Epoch 19/75\n",
            "778/782 [============================>.] - ETA: 0s - loss: 0.2306 - accuracy: 0.9140\n",
            "Epoch 00019: val_loss did not improve from 0.21002\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.2306 - accuracy: 0.9140 - val_loss: 0.2195 - val_accuracy: 0.9169\n",
            "Epoch 20/75\n",
            "771/782 [============================>.] - ETA: 0s - loss: 0.2329 - accuracy: 0.9142\n",
            "Epoch 00020: val_loss improved from 0.21002 to 0.20756, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2333 - accuracy: 0.9140 - val_loss: 0.2076 - val_accuracy: 0.9220\n",
            "Epoch 21/75\n",
            "772/782 [============================>.] - ETA: 0s - loss: 0.2253 - accuracy: 0.9148\n",
            "Epoch 00021: val_loss improved from 0.20756 to 0.20722, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.2248 - accuracy: 0.9150 - val_loss: 0.2072 - val_accuracy: 0.9224\n",
            "Epoch 22/75\n",
            "774/782 [============================>.] - ETA: 0s - loss: 0.2251 - accuracy: 0.9153\n",
            "Epoch 00022: val_loss did not improve from 0.20722\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.2248 - accuracy: 0.9153 - val_loss: 0.2200 - val_accuracy: 0.9160\n",
            "Epoch 23/75\n",
            "778/782 [============================>.] - ETA: 0s - loss: 0.2208 - accuracy: 0.9166\n",
            "Epoch 00023: val_loss did not improve from 0.20722\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.2206 - accuracy: 0.9166 - val_loss: 0.2073 - val_accuracy: 0.9233\n",
            "Epoch 24/75\n",
            "781/782 [============================>.] - ETA: 0s - loss: 0.2192 - accuracy: 0.9182\n",
            "Epoch 00024: val_loss did not improve from 0.20722\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.2191 - accuracy: 0.9183 - val_loss: 0.2121 - val_accuracy: 0.9193\n",
            "Epoch 25/75\n",
            "778/782 [============================>.] - ETA: 0s - loss: 0.2188 - accuracy: 0.9188\n",
            "Epoch 00025: val_loss improved from 0.20722 to 0.20464, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.2189 - accuracy: 0.9187 - val_loss: 0.2046 - val_accuracy: 0.9253\n",
            "Epoch 26/75\n",
            "772/782 [============================>.] - ETA: 0s - loss: 0.2104 - accuracy: 0.9219\n",
            "Epoch 00026: val_loss did not improve from 0.20464\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.2106 - accuracy: 0.9218 - val_loss: 0.2082 - val_accuracy: 0.9224\n",
            "Epoch 27/75\n",
            "780/782 [============================>.] - ETA: 0s - loss: 0.2108 - accuracy: 0.9210\n",
            "Epoch 00027: val_loss did not improve from 0.20464\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.2108 - accuracy: 0.9209 - val_loss: 0.2064 - val_accuracy: 0.9232\n",
            "Epoch 28/75\n",
            "775/782 [============================>.] - ETA: 0s - loss: 0.2085 - accuracy: 0.9223\n",
            "Epoch 00028: val_loss improved from 0.20464 to 0.20004, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.2084 - accuracy: 0.9224 - val_loss: 0.2000 - val_accuracy: 0.9247\n",
            "Epoch 29/75\n",
            "771/782 [============================>.] - ETA: 0s - loss: 0.2043 - accuracy: 0.9227\n",
            "Epoch 00029: val_loss did not improve from 0.20004\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.2041 - accuracy: 0.9228 - val_loss: 0.2108 - val_accuracy: 0.9223\n",
            "Epoch 30/75\n",
            "775/782 [============================>.] - ETA: 0s - loss: 0.2044 - accuracy: 0.9227\n",
            "Epoch 00030: val_loss did not improve from 0.20004\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.2042 - accuracy: 0.9228 - val_loss: 0.2023 - val_accuracy: 0.9235\n",
            "Epoch 31/75\n",
            "777/782 [============================>.] - ETA: 0s - loss: 0.2016 - accuracy: 0.9239\n",
            "Epoch 00031: val_loss did not improve from 0.20004\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.2015 - accuracy: 0.9240 - val_loss: 0.2054 - val_accuracy: 0.9222\n",
            "Epoch 32/75\n",
            "776/782 [============================>.] - ETA: 0s - loss: 0.2000 - accuracy: 0.9256\n",
            "Epoch 00032: val_loss did not improve from 0.20004\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.2000 - accuracy: 0.9257 - val_loss: 0.2052 - val_accuracy: 0.9233\n",
            "Epoch 33/75\n",
            "775/782 [============================>.] - ETA: 0s - loss: 0.1980 - accuracy: 0.9246\n",
            "Epoch 00033: val_loss did not improve from 0.20004\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1981 - accuracy: 0.9246 - val_loss: 0.2167 - val_accuracy: 0.9194\n",
            "Epoch 34/75\n",
            "775/782 [============================>.] - ETA: 0s - loss: 0.1992 - accuracy: 0.9257\n",
            "Epoch 00034: val_loss improved from 0.20004 to 0.19978, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.1990 - accuracy: 0.9258 - val_loss: 0.1998 - val_accuracy: 0.9264\n",
            "Epoch 35/75\n",
            "777/782 [============================>.] - ETA: 0s - loss: 0.1966 - accuracy: 0.9261\n",
            "Epoch 00035: val_loss did not improve from 0.19978\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1964 - accuracy: 0.9261 - val_loss: 0.2018 - val_accuracy: 0.9263\n",
            "Epoch 36/75\n",
            "771/782 [============================>.] - ETA: 0s - loss: 0.1930 - accuracy: 0.9257\n",
            "Epoch 00036: val_loss did not improve from 0.19978\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1929 - accuracy: 0.9257 - val_loss: 0.2009 - val_accuracy: 0.9261\n",
            "Epoch 37/75\n",
            "770/782 [============================>.] - ETA: 0s - loss: 0.1958 - accuracy: 0.9281\n",
            "Epoch 00037: val_loss did not improve from 0.19978\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1957 - accuracy: 0.9281 - val_loss: 0.2085 - val_accuracy: 0.9225\n",
            "Epoch 38/75\n",
            "779/782 [============================>.] - ETA: 0s - loss: 0.1916 - accuracy: 0.9277\n",
            "Epoch 00038: val_loss did not improve from 0.19978\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.1915 - accuracy: 0.9277 - val_loss: 0.2106 - val_accuracy: 0.9198\n",
            "Epoch 39/75\n",
            "777/782 [============================>.] - ETA: 0s - loss: 0.1906 - accuracy: 0.9277\n",
            "Epoch 00039: val_loss did not improve from 0.19978\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1904 - accuracy: 0.9277 - val_loss: 0.2039 - val_accuracy: 0.9234\n",
            "Epoch 40/75\n",
            "775/782 [============================>.] - ETA: 0s - loss: 0.1930 - accuracy: 0.9278\n",
            "Epoch 00040: val_loss improved from 0.19978 to 0.19934, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.1934 - accuracy: 0.9277 - val_loss: 0.1993 - val_accuracy: 0.9280\n",
            "Epoch 41/75\n",
            "773/782 [============================>.] - ETA: 0s - loss: 0.1908 - accuracy: 0.9276\n",
            "Epoch 00041: val_loss did not improve from 0.19934\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1906 - accuracy: 0.9276 - val_loss: 0.2147 - val_accuracy: 0.9222\n",
            "Epoch 42/75\n",
            "781/782 [============================>.] - ETA: 0s - loss: 0.1854 - accuracy: 0.9292\n",
            "Epoch 00042: val_loss improved from 0.19934 to 0.19654, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.1854 - accuracy: 0.9292 - val_loss: 0.1965 - val_accuracy: 0.9274\n",
            "Epoch 43/75\n",
            "776/782 [============================>.] - ETA: 0s - loss: 0.1839 - accuracy: 0.9295\n",
            "Epoch 00043: val_loss did not improve from 0.19654\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.1839 - accuracy: 0.9295 - val_loss: 0.2033 - val_accuracy: 0.9266\n",
            "Epoch 44/75\n",
            "781/782 [============================>.] - ETA: 0s - loss: 0.1806 - accuracy: 0.9314\n",
            "Epoch 00044: val_loss did not improve from 0.19654\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.1806 - accuracy: 0.9314 - val_loss: 0.2065 - val_accuracy: 0.9259\n",
            "Epoch 45/75\n",
            "777/782 [============================>.] - ETA: 0s - loss: 0.1819 - accuracy: 0.9300\n",
            "Epoch 00045: val_loss did not improve from 0.19654\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.1819 - accuracy: 0.9299 - val_loss: 0.2069 - val_accuracy: 0.9243\n",
            "Epoch 46/75\n",
            "778/782 [============================>.] - ETA: 0s - loss: 0.1843 - accuracy: 0.9306\n",
            "Epoch 00046: val_loss did not improve from 0.19654\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1843 - accuracy: 0.9305 - val_loss: 0.2021 - val_accuracy: 0.9266\n",
            "Epoch 47/75\n",
            "779/782 [============================>.] - ETA: 0s - loss: 0.1812 - accuracy: 0.9317\n",
            "Epoch 00047: val_loss did not improve from 0.19654\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1811 - accuracy: 0.9317 - val_loss: 0.2129 - val_accuracy: 0.9224\n",
            "Epoch 48/75\n",
            "772/782 [============================>.] - ETA: 0s - loss: 0.1825 - accuracy: 0.9308\n",
            "Epoch 00048: val_loss improved from 0.19654 to 0.19615, saving model to model.weights.best.hdf5\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1831 - accuracy: 0.9306 - val_loss: 0.1961 - val_accuracy: 0.9275\n",
            "Epoch 49/75\n",
            "770/782 [============================>.] - ETA: 0s - loss: 0.1768 - accuracy: 0.9327\n",
            "Epoch 00049: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1770 - accuracy: 0.9327 - val_loss: 0.2090 - val_accuracy: 0.9242\n",
            "Epoch 50/75\n",
            "773/782 [============================>.] - ETA: 0s - loss: 0.1779 - accuracy: 0.9322\n",
            "Epoch 00050: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1783 - accuracy: 0.9320 - val_loss: 0.2262 - val_accuracy: 0.9188\n",
            "Epoch 51/75\n",
            "775/782 [============================>.] - ETA: 0s - loss: 0.1773 - accuracy: 0.9322\n",
            "Epoch 00051: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1772 - accuracy: 0.9322 - val_loss: 0.1996 - val_accuracy: 0.9280\n",
            "Epoch 52/75\n",
            "780/782 [============================>.] - ETA: 0s - loss: 0.1769 - accuracy: 0.9344\n",
            "Epoch 00052: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1770 - accuracy: 0.9344 - val_loss: 0.2036 - val_accuracy: 0.9250\n",
            "Epoch 53/75\n",
            "776/782 [============================>.] - ETA: 0s - loss: 0.1773 - accuracy: 0.9325\n",
            "Epoch 00053: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1768 - accuracy: 0.9327 - val_loss: 0.2044 - val_accuracy: 0.9268\n",
            "Epoch 54/75\n",
            "773/782 [============================>.] - ETA: 0s - loss: 0.1711 - accuracy: 0.9349\n",
            "Epoch 00054: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1709 - accuracy: 0.9349 - val_loss: 0.2069 - val_accuracy: 0.9252\n",
            "Epoch 55/75\n",
            "779/782 [============================>.] - ETA: 0s - loss: 0.1734 - accuracy: 0.9341\n",
            "Epoch 00055: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1734 - accuracy: 0.9340 - val_loss: 0.2070 - val_accuracy: 0.9240\n",
            "Epoch 56/75\n",
            "775/782 [============================>.] - ETA: 0s - loss: 0.1707 - accuracy: 0.9349\n",
            "Epoch 00056: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1706 - accuracy: 0.9350 - val_loss: 0.2089 - val_accuracy: 0.9245\n",
            "Epoch 57/75\n",
            "770/782 [============================>.] - ETA: 0s - loss: 0.1725 - accuracy: 0.9356\n",
            "Epoch 00057: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.1726 - accuracy: 0.9355 - val_loss: 0.1978 - val_accuracy: 0.9278\n",
            "Epoch 58/75\n",
            "775/782 [============================>.] - ETA: 0s - loss: 0.1734 - accuracy: 0.9353\n",
            "Epoch 00058: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1732 - accuracy: 0.9354 - val_loss: 0.1995 - val_accuracy: 0.9277\n",
            "Epoch 59/75\n",
            "772/782 [============================>.] - ETA: 0s - loss: 0.1733 - accuracy: 0.9354\n",
            "Epoch 00059: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.1735 - accuracy: 0.9353 - val_loss: 0.2038 - val_accuracy: 0.9246\n",
            "Epoch 60/75\n",
            "771/782 [============================>.] - ETA: 0s - loss: 0.1707 - accuracy: 0.9359\n",
            "Epoch 00060: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.1708 - accuracy: 0.9360 - val_loss: 0.2000 - val_accuracy: 0.9271\n",
            "Epoch 61/75\n",
            "773/782 [============================>.] - ETA: 0s - loss: 0.1666 - accuracy: 0.9360\n",
            "Epoch 00061: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1662 - accuracy: 0.9362 - val_loss: 0.2032 - val_accuracy: 0.9277\n",
            "Epoch 62/75\n",
            "772/782 [============================>.] - ETA: 0s - loss: 0.1669 - accuracy: 0.9361\n",
            "Epoch 00062: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1667 - accuracy: 0.9361 - val_loss: 0.2034 - val_accuracy: 0.9252\n",
            "Epoch 63/75\n",
            "781/782 [============================>.] - ETA: 0s - loss: 0.1696 - accuracy: 0.9345\n",
            "Epoch 00063: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1696 - accuracy: 0.9345 - val_loss: 0.2069 - val_accuracy: 0.9231\n",
            "Epoch 64/75\n",
            "778/782 [============================>.] - ETA: 0s - loss: 0.1637 - accuracy: 0.9385\n",
            "Epoch 00064: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1637 - accuracy: 0.9386 - val_loss: 0.2018 - val_accuracy: 0.9250\n",
            "Epoch 65/75\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.1651 - accuracy: 0.9381\n",
            "Epoch 00065: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1651 - accuracy: 0.9381 - val_loss: 0.1974 - val_accuracy: 0.9301\n",
            "Epoch 66/75\n",
            "772/782 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.9379\n",
            "Epoch 00066: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1655 - accuracy: 0.9378 - val_loss: 0.2007 - val_accuracy: 0.9280\n",
            "Epoch 67/75\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.1652 - accuracy: 0.9376\n",
            "Epoch 00067: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1652 - accuracy: 0.9376 - val_loss: 0.1969 - val_accuracy: 0.9277\n",
            "Epoch 68/75\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.1646 - accuracy: 0.9379\n",
            "Epoch 00068: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1646 - accuracy: 0.9379 - val_loss: 0.2065 - val_accuracy: 0.9285\n",
            "Epoch 69/75\n",
            "772/782 [============================>.] - ETA: 0s - loss: 0.1661 - accuracy: 0.9377\n",
            "Epoch 00069: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1658 - accuracy: 0.9378 - val_loss: 0.2019 - val_accuracy: 0.9294\n",
            "Epoch 70/75\n",
            "775/782 [============================>.] - ETA: 0s - loss: 0.1628 - accuracy: 0.9379\n",
            "Epoch 00070: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1624 - accuracy: 0.9381 - val_loss: 0.2045 - val_accuracy: 0.9296\n",
            "Epoch 71/75\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.1612 - accuracy: 0.9393\n",
            "Epoch 00071: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.1612 - accuracy: 0.9393 - val_loss: 0.2134 - val_accuracy: 0.9257\n",
            "Epoch 72/75\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.1647 - accuracy: 0.9383\n",
            "Epoch 00072: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1647 - accuracy: 0.9383 - val_loss: 0.1976 - val_accuracy: 0.9291\n",
            "Epoch 73/75\n",
            "776/782 [============================>.] - ETA: 0s - loss: 0.1642 - accuracy: 0.9384\n",
            "Epoch 00073: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1648 - accuracy: 0.9381 - val_loss: 0.2027 - val_accuracy: 0.9292\n",
            "Epoch 74/75\n",
            "779/782 [============================>.] - ETA: 0s - loss: 0.1609 - accuracy: 0.9387\n",
            "Epoch 00074: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1608 - accuracy: 0.9388 - val_loss: 0.2112 - val_accuracy: 0.9269\n",
            "Epoch 75/75\n",
            "780/782 [============================>.] - ETA: 0s - loss: 0.1645 - accuracy: 0.9378\n",
            "Epoch 00075: val_loss did not improve from 0.19615\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.1644 - accuracy: 0.9378 - val_loss: 0.2055 - val_accuracy: 0.9275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8a8b208710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5FgTRP6yIV4",
        "outputId": "70ae66fc-f5d0-46fc-ed24-ba8e4c65f86e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.load_weights('model.weights.best.hdf5')\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "# Printing the test accuracy\n",
        "print('\\n', 'Test accuracy:', score[1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Test accuracy: 0.9236999750137329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfTL_5kPsl4j"
      },
      "source": [
        "### Task 1.2 Train a ConvNet from scratch\n",
        "\n",
        "*(weight ~2%)*\n",
        "\n",
        "Build a ConvNet to replace the densely connected network in Task 1.1. Report the classification accuracy on the test set. Aim to achieve higher accuracy. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1V9w6QwyL1A"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Reshaping input data from (28, 28) to (28, 28, 1)\n",
        "w, h = 28, 28\n",
        "x_train = x_train.reshape(x_train.shape[0], w, h, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], w, h, 1)\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "#we rescale the model for smoothness \n",
        "x_train = x_train.astype('float32')/255.0\n",
        "x_test = x_test.astype('float32')/255.0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wIBM5YzyOAo",
        "outputId": "b62b371c-c52f-4ea3-bb00-654bb0db4852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from numpy import mean\n",
        "import pandas as pd\n",
        "\n",
        "scores, histories = list(), list()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#select rows for train and test\n",
        "history = model.fit(x_train, y_train, validation_split=0.2, epochs=50)\n",
        "  \n",
        "#Model Evaluation\n",
        "_, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('> %.3f' % (acc * 100.0))\n",
        " \n",
        "#storing scores\n",
        "scores.append(acc)\n",
        "histories.append(history)\n",
        "print(\"Accuracy:\",acc*100)\n",
        "\n",
        "comparison_df = pd.DataFrame(columns=['configuration','accuracy'])\n",
        "row={'configuration': 'ConvNet', 'accuracy': acc*100}\n",
        "comparison_df=comparison_df.append(row,ignore_index=True)\n",
        "\n",
        "final_comparison_df = pd.DataFrame(columns=['configuration','accuracy'])\n",
        "row={'configuration': 'ConvNet', 'accuracy': acc*100}\n",
        "final_comparison_df=final_comparison_df.append(row,ignore_index=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.4119 - accuracy: 0.8533 - val_loss: 0.3245 - val_accuracy: 0.8852\n",
            "Epoch 2/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2734 - accuracy: 0.9021 - val_loss: 0.2776 - val_accuracy: 0.8991\n",
            "Epoch 3/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2220 - accuracy: 0.9197 - val_loss: 0.2724 - val_accuracy: 0.9038\n",
            "Epoch 4/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1796 - accuracy: 0.9352 - val_loss: 0.2706 - val_accuracy: 0.9078\n",
            "Epoch 5/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1456 - accuracy: 0.9470 - val_loss: 0.2770 - val_accuracy: 0.9093\n",
            "Epoch 6/50\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1183 - accuracy: 0.9574 - val_loss: 0.2850 - val_accuracy: 0.9091\n",
            "Epoch 7/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0949 - accuracy: 0.9658 - val_loss: 0.3209 - val_accuracy: 0.9090\n",
            "Epoch 8/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0812 - accuracy: 0.9707 - val_loss: 0.3359 - val_accuracy: 0.9070\n",
            "Epoch 9/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0653 - accuracy: 0.9759 - val_loss: 0.3720 - val_accuracy: 0.9120\n",
            "Epoch 10/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0536 - accuracy: 0.9806 - val_loss: 0.4021 - val_accuracy: 0.9095\n",
            "Epoch 11/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0460 - accuracy: 0.9836 - val_loss: 0.4451 - val_accuracy: 0.9089\n",
            "Epoch 12/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0397 - accuracy: 0.9857 - val_loss: 0.4685 - val_accuracy: 0.9085\n",
            "Epoch 13/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0374 - accuracy: 0.9868 - val_loss: 0.5227 - val_accuracy: 0.9045\n",
            "Epoch 14/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0290 - accuracy: 0.9899 - val_loss: 0.5499 - val_accuracy: 0.9043\n",
            "Epoch 15/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0284 - accuracy: 0.9892 - val_loss: 0.5470 - val_accuracy: 0.9013\n",
            "Epoch 16/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0230 - accuracy: 0.9916 - val_loss: 0.6293 - val_accuracy: 0.9022\n",
            "Epoch 17/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0235 - accuracy: 0.9912 - val_loss: 0.6339 - val_accuracy: 0.9009\n",
            "Epoch 18/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0201 - accuracy: 0.9928 - val_loss: 0.6918 - val_accuracy: 0.9006\n",
            "Epoch 19/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0188 - accuracy: 0.9927 - val_loss: 0.7150 - val_accuracy: 0.9053\n",
            "Epoch 20/50\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.0166 - accuracy: 0.9939 - val_loss: 0.7431 - val_accuracy: 0.9024\n",
            "Epoch 21/50\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0200 - accuracy: 0.9927 - val_loss: 0.7199 - val_accuracy: 0.8969\n",
            "Epoch 22/50\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.0152 - accuracy: 0.9948 - val_loss: 0.7671 - val_accuracy: 0.8966\n",
            "Epoch 23/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0155 - accuracy: 0.9944 - val_loss: 0.7826 - val_accuracy: 0.8994\n",
            "Epoch 24/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0146 - accuracy: 0.9946 - val_loss: 0.8505 - val_accuracy: 0.9002\n",
            "Epoch 25/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0164 - accuracy: 0.9944 - val_loss: 0.8458 - val_accuracy: 0.9004\n",
            "Epoch 26/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0141 - accuracy: 0.9950 - val_loss: 0.9367 - val_accuracy: 0.8985\n",
            "Epoch 27/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0132 - accuracy: 0.9952 - val_loss: 0.9294 - val_accuracy: 0.8984\n",
            "Epoch 28/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0151 - accuracy: 0.9949 - val_loss: 0.8924 - val_accuracy: 0.8958\n",
            "Epoch 29/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0106 - accuracy: 0.9962 - val_loss: 0.9350 - val_accuracy: 0.9017\n",
            "Epoch 30/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0122 - accuracy: 0.9959 - val_loss: 0.9665 - val_accuracy: 0.8990\n",
            "Epoch 31/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0136 - accuracy: 0.9952 - val_loss: 0.9761 - val_accuracy: 0.9008\n",
            "Epoch 32/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0102 - accuracy: 0.9960 - val_loss: 0.9735 - val_accuracy: 0.8994\n",
            "Epoch 33/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0111 - accuracy: 0.9960 - val_loss: 1.0909 - val_accuracy: 0.9021\n",
            "Epoch 34/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0134 - accuracy: 0.9952 - val_loss: 1.0589 - val_accuracy: 0.9022\n",
            "Epoch 35/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0087 - accuracy: 0.9970 - val_loss: 1.1484 - val_accuracy: 0.8999\n",
            "Epoch 36/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0101 - accuracy: 0.9962 - val_loss: 1.1762 - val_accuracy: 0.8958\n",
            "Epoch 37/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0118 - accuracy: 0.9960 - val_loss: 1.1486 - val_accuracy: 0.9010\n",
            "Epoch 38/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0118 - accuracy: 0.9959 - val_loss: 1.0906 - val_accuracy: 0.8992\n",
            "Epoch 39/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0097 - accuracy: 0.9964 - val_loss: 1.1600 - val_accuracy: 0.8959\n",
            "Epoch 40/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0105 - accuracy: 0.9963 - val_loss: 1.1016 - val_accuracy: 0.9003\n",
            "Epoch 41/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0091 - accuracy: 0.9969 - val_loss: 1.1258 - val_accuracy: 0.8964\n",
            "Epoch 42/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0111 - accuracy: 0.9962 - val_loss: 1.1602 - val_accuracy: 0.8978\n",
            "Epoch 43/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0083 - accuracy: 0.9972 - val_loss: 1.2141 - val_accuracy: 0.8988\n",
            "Epoch 44/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0123 - accuracy: 0.9962 - val_loss: 1.1926 - val_accuracy: 0.8991\n",
            "Epoch 45/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0099 - accuracy: 0.9967 - val_loss: 1.2576 - val_accuracy: 0.9003\n",
            "Epoch 46/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0063 - accuracy: 0.9977 - val_loss: 1.2539 - val_accuracy: 0.9021\n",
            "Epoch 47/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0109 - accuracy: 0.9959 - val_loss: 1.2884 - val_accuracy: 0.9033\n",
            "Epoch 48/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0092 - accuracy: 0.9970 - val_loss: 1.2344 - val_accuracy: 0.8977\n",
            "Epoch 49/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0089 - accuracy: 0.9968 - val_loss: 1.2277 - val_accuracy: 0.8987\n",
            "Epoch 50/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0092 - accuracy: 0.9969 - val_loss: 1.4090 - val_accuracy: 0.8970\n",
            "> 88.930\n",
            "Accuracy: 88.9299988746643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbhr44DnMbKZ"
      },
      "source": [
        "\n",
        "### Task 1.3 Build an input pipeline for data augmentation\n",
        "\n",
        "*(weight ~4%)*\n",
        "\n",
        "Build a data preprocessing pipeline to perform data augmentation. (You may use Keras ImageDataGenerator or write your own transformations.)\n",
        "\n",
        "- Report the new classification accuracy. Make sure that you use the same number of training epochs as in Task 1.2.\n",
        "\n",
        "- (Optional) Profile your input pipeline to identify the most time-consuming operation. What actions have you taken to address that slow operation? (*Hint: You may use the [TensorFlow Profiler](https://github.com/tensorflow/profiler).*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SQohhfLyR9o",
        "outputId": "ccd7afd4-33f6-44ca-fb4e-3ace555dccf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot\n",
        "\n",
        "scores, histories = list(), list()\n",
        "\n",
        "train_datagen = ImageDataGenerator(rotation_range=90, # rotating\n",
        "                                  width_shift_range=0.2, # horizontal shifting\n",
        "                                  height_shift_range=0.2, # vertical shifting\n",
        "                                  horizontal_flip=True, # horizontal fliping\n",
        "                                  vertical_flip=True, #horizontal fliping\n",
        "                                  brightness_range=[0.2,1.2]) # marking brightness)\n",
        "\n",
        "(x_train, x_valid) = x_train[10000:], x_train[:10000]\n",
        "(y_train, y_valid) = y_train[10000:], y_train[:10000]\n",
        "\n",
        "#Training using ImageDataGenerator\n",
        "#Same model used as above (Task 1.2)\n",
        "history = model.fit(train_datagen.flow(x_train, y_train,\n",
        "                                     batch_size=32),\n",
        "                    epochs=50,\n",
        "                    validation_data=(x_valid,y_valid))\n",
        "\n",
        "#Model Evaluation\n",
        "_, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Accuracy\",acc*100)\n",
        "\n",
        "#storing scores\n",
        "scores.append(acc)\n",
        "histories.append(history)\n",
        "\n",
        "row={'configuration': 'ImageDataGenerator', 'accuracy': acc*100}\n",
        "comparison_df=comparison_df.append(row,ignore_index=True)\n",
        "\n",
        "row={'configuration': 'ImageDataGenerator', 'accuracy': acc*100}\n",
        "final_comparison_df=final_comparison_df.append(row,ignore_index=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 14.8818 - accuracy: 0.0979 - val_loss: 2.3020 - val_accuracy: 0.1015\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3077 - accuracy: 0.0978 - val_loss: 2.3027 - val_accuracy: 0.1022\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3044 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.0988\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3031 - accuracy: 0.1002 - val_loss: 2.3028 - val_accuracy: 0.0942\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 2.3028 - accuracy: 0.0991 - val_loss: 2.3027 - val_accuracy: 0.1019\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3030 - accuracy: 0.1018 - val_loss: 2.3027 - val_accuracy: 0.0974\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3029 - accuracy: 0.0970 - val_loss: 2.3027 - val_accuracy: 0.0974\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3029 - accuracy: 0.0984 - val_loss: 2.3028 - val_accuracy: 0.0989\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0971 - val_loss: 2.3027 - val_accuracy: 0.1022\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0985 - val_loss: 2.3030 - val_accuracy: 0.0942\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0969 - val_loss: 2.3028 - val_accuracy: 0.0942\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 2.3028 - accuracy: 0.0992 - val_loss: 2.3027 - val_accuracy: 0.0942\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3027 - accuracy: 0.0998 - val_loss: 2.3029 - val_accuracy: 0.0990\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0995 - val_loss: 2.3028 - val_accuracy: 0.0989\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 2.3028 - accuracy: 0.0984 - val_loss: 2.3030 - val_accuracy: 0.0990\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3027 - accuracy: 0.1002 - val_loss: 2.3028 - val_accuracy: 0.0990\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0996 - val_loss: 2.3027 - val_accuracy: 0.0942\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0982 - val_loss: 2.3030 - val_accuracy: 0.0942\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3027 - accuracy: 0.0973 - val_loss: 2.3028 - val_accuracy: 0.0942\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 2.3028 - accuracy: 0.0990 - val_loss: 2.3027 - val_accuracy: 0.0942\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.1001 - val_loss: 2.3029 - val_accuracy: 0.0942\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0978 - val_loss: 2.3030 - val_accuracy: 0.0974\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0983 - val_loss: 2.3027 - val_accuracy: 0.1021\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0974 - val_loss: 2.3029 - val_accuracy: 0.0990\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 2.3027 - accuracy: 0.1000 - val_loss: 2.3028 - val_accuracy: 0.0990\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0962 - val_loss: 2.3027 - val_accuracy: 0.0974\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3027 - accuracy: 0.0988 - val_loss: 2.3027 - val_accuracy: 0.1021\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 2.3028 - accuracy: 0.0984 - val_loss: 2.3026 - val_accuracy: 0.0989\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 2.3028 - accuracy: 0.0974 - val_loss: 2.3028 - val_accuracy: 0.0942\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 2.3027 - accuracy: 0.0990 - val_loss: 2.3030 - val_accuracy: 0.0942\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 2.3028 - accuracy: 0.1003 - val_loss: 2.3027 - val_accuracy: 0.1019\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0986 - val_loss: 2.3026 - val_accuracy: 0.0974\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0965 - val_loss: 2.3029 - val_accuracy: 0.0990\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 2.3028 - accuracy: 0.0986 - val_loss: 2.3027 - val_accuracy: 0.0942\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.1006 - val_loss: 2.3027 - val_accuracy: 0.0990\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0985 - val_loss: 2.3029 - val_accuracy: 0.0990\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 2.3028 - accuracy: 0.1000 - val_loss: 2.3028 - val_accuracy: 0.0942\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 2.3028 - accuracy: 0.1003 - val_loss: 2.3026 - val_accuracy: 0.1027\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 2.3028 - accuracy: 0.0975 - val_loss: 2.3027 - val_accuracy: 0.1019\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0965 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0980 - val_loss: 2.3028 - val_accuracy: 0.0974\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0974 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3027 - accuracy: 0.0982 - val_loss: 2.3027 - val_accuracy: 0.1019\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 2.3028 - accuracy: 0.0988 - val_loss: 2.3029 - val_accuracy: 0.0942\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3027 - accuracy: 0.0987 - val_loss: 2.3031 - val_accuracy: 0.0974\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0994 - val_loss: 2.3029 - val_accuracy: 0.0942\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0976 - val_loss: 2.3028 - val_accuracy: 0.0974\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3027 - accuracy: 0.0997 - val_loss: 2.3025 - val_accuracy: 0.1016\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0977 - val_loss: 2.3028 - val_accuracy: 0.0989\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3028 - accuracy: 0.0986 - val_loss: 2.3028 - val_accuracy: 0.0942\n",
            "Accuracy 10.000000149011612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prctXU4BswKK"
      },
      "source": [
        "### Task 1.4 Fashion-MNIST with transfer learning\n",
        "\n",
        "*(weight ~6%)*\n",
        "\n",
        "Use a pretrained model as the convolutional base to improve the classification performance. (Hint: You may use models in Keras Applications or those in the TensorFlow Hub.)\n",
        "\n",
        "- Try both with fine-tuning and without fine-tuning.\n",
        "- Report the model performance as before.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL0yyweSyUgJ"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow import keras\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# transform to rgb as required by VGG\n",
        "train_img=tf.image.grayscale_to_rgb(tf.expand_dims(x_train, axis=3)) \n",
        "test_img=tf.image.grayscale_to_rgb(tf.expand_dims(x_test, axis=3))\n",
        "\n",
        "#resize to minimum size of (32x32)\n",
        "train_img=tf.image.resize_with_pad(train_img,32,32)\n",
        "test_img=tf.image.resize_with_pad(test_img,32,32)\n",
        "\n",
        "train_img = train_img / 255.\n",
        "test_img = test_img / 255.\n",
        "\n",
        "#preprocessing as required by VGG16\n",
        "train_img=preprocess_input(train_img)\n",
        "test_img=preprocess_input(test_img)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEAp6iubyWwS",
        "outputId": "77f7fbb8-ffc6-4b7f-a791-75d427556af1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#With Fine Tuning\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "#using model without last layers\n",
        "vgg16=VGG16(include_top=False, weights='imagenet', input_shape=(32,32,3))\n",
        "\n",
        "layer_dict = dict([(layer.name, layer) for layer in vgg16.layers])\n",
        "\n",
        "#stop at block3_pool and get output\n",
        "output = layer_dict['block3_pool'].output\n",
        "\n",
        "x = keras.layers.Flatten()(output)\n",
        "x = keras.layers.Dense(32, activation='relu')(x)\n",
        "x = keras.layers.Dense(10)(x)\n",
        "\n",
        "final = keras.models.Model(inputs=vgg16.input, outputs=x)\n",
        "for layer in final.layers[:10]:\n",
        "  layer.trainable = False\n",
        "\n",
        "final.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "final.fit(train_img, y_train, epochs=50, validation_split=0.2,batch_size=256,)\n",
        "\n",
        "#Model Evaluation\n",
        "_, acc = final.evaluate(test_img, y_test, verbose=0)\n",
        "print('Accuracy: %.3f' % (acc*100))\n",
        "\n",
        "row={'configuration': 'Transfer Learning with Fine Tuning', 'accuracy': acc*100}\n",
        "comparison_df=comparison_df.append(row,ignore_index=True)\n",
        "\n",
        "row={'configuration': 'Transfer Learning with Fine Tuning', 'accuracy': acc*100}\n",
        "final_comparison_df=final_comparison_df.append(row,ignore_index=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "Epoch 1/50\n",
            "188/188 [==============================] - 3s 15ms/step - loss: 12.3160 - accuracy: 0.1000 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 2/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 3/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3866 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 4/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 5/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 6/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 7/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3866 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 8/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 9/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3866 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 10/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 11/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 12/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 13/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 14/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 15/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 16/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 17/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3866 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 18/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3866 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 19/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 20/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3866 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 21/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 22/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3866 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 23/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3866 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 24/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 25/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3866 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 26/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 27/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 28/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 29/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 30/50\n",
            "188/188 [==============================] - 2s 13ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 31/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3866 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 32/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3866 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 33/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 34/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 35/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 36/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 37/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 38/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3866 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 39/50\n",
            "188/188 [==============================] - 2s 13ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 40/50\n",
            "188/188 [==============================] - 2s 13ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 41/50\n",
            "188/188 [==============================] - 2s 13ms/step - loss: 12.3866 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 42/50\n",
            "188/188 [==============================] - 2s 13ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 43/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 44/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 45/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 46/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3866 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 47/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 48/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3866 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 49/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Epoch 50/50\n",
            "188/188 [==============================] - 2s 12ms/step - loss: 12.3867 - accuracy: 0.1004 - val_loss: 12.3598 - val_accuracy: 0.0983\n",
            "Accuracy: 10.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCJrZTBxyZbQ",
        "outputId": "fc9f6082-c410-4c8a-830d-cf2b6225e343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Without fine tuning\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "base_model = VGG16(\n",
        "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=(32, 32, 3),\n",
        "    include_top=False)\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "# We make sure that the base_model is running in inference mode here,\n",
        "# by passing `training=False`.\n",
        "x = base_model(inputs, training=False)\n",
        "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "# A Dense classifier\n",
        "outputs = keras.layers.Dense(10)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_img, y_train, epochs=50, validation_split=0.2)\n",
        "\n",
        "#Model Evaluation\n",
        "_, acc = final.evaluate(test_img, y_test, verbose=0)\n",
        "print('Accuracy: %.3f' % (acc*100))\n",
        "\n",
        "row={'configuration': 'Transfer Learning without Fine Tuning', 'accuracy': acc*100}\n",
        "comparison_df=comparison_df.append(row,ignore_index=True)\n",
        "\n",
        "row={'configuration': 'Transfer Learning without Fine Tuning', 'accuracy': acc*100}\n",
        "final_comparison_df=final_comparison_df.append(row,ignore_index=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 2/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 3/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 4/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 5/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 6/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 7/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 8/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 9/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 10/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 11/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 12/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 13/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 14/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 15/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 16/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 17/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 18/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 19/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 20/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 21/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 22/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 23/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 24/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 25/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 26/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 27/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 28/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 29/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 30/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 31/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 32/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 33/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 34/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 35/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 36/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 37/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 38/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 39/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 40/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 41/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 42/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 43/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 44/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 45/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 46/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 47/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 48/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 49/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Epoch 50/50\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 5.2859 - accuracy: 0.1011 - val_loss: 5.3716 - val_accuracy: 0.0957\n",
            "Accuracy: 10.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaHLKDLas_dF"
      },
      "source": [
        "### Task 1.5 Performance comparison\n",
        "\n",
        "*(weight ~4%)*\n",
        "\n",
        "Record the test accuracy achieved at different training configurations above. Which method achieved the highest accuracy? Why did it work better for this problem?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yCozYbpycEL",
        "outputId": "c51d6f9f-1bf9-4cf8-8d29-04ef89b4e113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "comparison_df"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>configuration</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ConvNet</td>\n",
              "      <td>88.929999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ImageDataGenerator</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Transfer Learning with Fine Tuning</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Transfer Learning without Fine Tuning</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           configuration   accuracy\n",
              "0                                ConvNet  88.929999\n",
              "1                     ImageDataGenerator  10.000000\n",
              "2     Transfer Learning with Fine Tuning  10.000000\n",
              "3  Transfer Learning without Fine Tuning  10.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJtWB3yuynKh",
        "outputId": "3df8344a-0389-483b-ba69-7e7ffc0663ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "highest=comparison_df['accuracy'].max()\n",
        "print(\"The highest accuracy is observed for:\\n\")\n",
        "comparison_df[comparison_df['accuracy']==highest]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The highest accuracy is observed for:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>configuration</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ConvNet</td>\n",
              "      <td>88.929999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  configuration   accuracy\n",
              "0       ConvNet  88.929999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMFHLzMRypML",
        "outputId": "8e131833-2a0e-4304-b0c8-9048ff02fc7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"CNN is considered to perform best because of it's architecture. \\nThe system first generates filtered invariant features using convolution of images and then passes it to the next layer \")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN is considered to perform best because of it's architecture. \n",
            "The system first generates filtered invariant features using convolution of images and then passes it to the next layer \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie2AUNOqycrB"
      },
      "source": [
        "## **End of task 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouK5NY-_pLDK"
      },
      "source": [
        "## Task 2 Fast training of deep networks\n",
        "\n",
        "*(weight ~16%)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgoOE2W1pdfN"
      },
      "source": [
        "###### Task 2.1 Train a highly accurate network for CIFAR10\n",
        "\n",
        "*(weight ~6%, each subquestion worths ~2%)*\n",
        "\n",
        "In this task, you will train deep neural networks on the [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). Compared with the datasets that you have worked on so far, CIFAR10 represents a relatively larger multi-class classification problem and presents a great opportunity for you to solve a \"harder\" problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaD5oqj3lhuI"
      },
      "source": [
        "#### Task 2.1.1 Document the hardware used\n",
        "\n",
        "Before you start, write down your hardware specifications, including \n",
        "\n",
        "- the GPU model, the number of GPUs, and the GPU memory\n",
        "- the CPU model, the number of CPUs, and the CPU clock speed\n",
        "\n",
        "(Hint: you may find commands like `nvidia-smi`, `lscpu` or `psutil` useful.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8QjBd-XywAQ",
        "outputId": "15c6ad39-d44f-4558-8683-577064a11284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "!pip install gputil\n",
        "import GPUtil\n",
        "from tabulate import tabulate\n",
        "print(\"=\"*40, \"GPU Details\", \"=\"*40)\n",
        "gpus = GPUtil.getGPUs()\n",
        "list_gpus = []\n",
        "count = 0\n",
        "for gpu in gpus:\n",
        "    print(\"GPU Number:\", count+1)\n",
        "    # name of GPU\n",
        "    print(\"GPU Name:\",gpu.name)\n",
        "    # get free memory in MB format\n",
        "    print(\"GPU Free Memory:\", gpu.memoryFree)\n",
        "    # get used memory\n",
        "    print(\"GPU Used Memory:\", gpu.memoryUsed)\n",
        "    # get total memory\n",
        "    print(\"GPU Total Memory:\", gpu.memoryTotal)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7411 sha256=bfa3625c7de14305bda07b3e1a2165146041eb1f113436be08ec5bcf0b08d58b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "======================================== GPU Details ========================================\n",
            "GPU Number: 1\n",
            "GPU Name: Tesla P100-PCIE-16GB\n",
            "GPU Free Memory: 11479.0\n",
            "GPU Used Memory: 4801.0\n",
            "GPU Total Memory: 16280.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXuCL8AUy28h",
        "outputId": "2d675fb6-8e91-499d-e9c9-d7c10d95e3ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install psutil\n",
        "import platform\n",
        "import psutil\n",
        "\n",
        "print(\"=\"*40, \"CPU Info\", \"=\"*40)\n",
        "#processor name\n",
        "print(\"Processor Name: \",platform.processor())\n",
        "# number of cores\n",
        "print(\"Total cores:\", psutil.cpu_count(logical=True))\n",
        "# CPU frequencies\n",
        "print(\"CPU Frequency: \",psutil.cpu_freq())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "======================================== CPU Info ========================================\n",
            "Processor Name:  x86_64\n",
            "Total cores: 4\n",
            "CPU Frequency:  None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adN9Tq-6lyG-"
      },
      "source": [
        "#### Task 2.1.2 Train a \"shallow\" ConvNet\n",
        "\n",
        "Build a ConvNet with fewer than 10 layers. Train the network until it converges. You will use this network as a baseline for the later experiments. \n",
        "\n",
        "- Plot the training and validation history. \n",
        "- Report the testing accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29p-PhobzMlA",
        "outputId": "d2b3962f-bc0f-481c-9195-ba65f3783ed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "(x_train_c, y_train_c), (x_test_c, y_test_c) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "y_train_c = tf.keras.utils.to_categorical(y_train_c)\n",
        "y_test_c = tf.keras.utils.to_categorical(y_test_c)\n",
        "\n",
        "#we rescale the model for smoothness \n",
        "x_train_c = x_train_c.astype('float32')/255.0\n",
        "x_test_c = x_test_c.astype('float32')/255.0"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJi6Bwz7zPtn",
        "outputId": "18c1adb2-7c76-431d-fe3b-bef6dd67bd69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=2, activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(Conv2D(filters=32, kernel_size=2, activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=2, activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(filters=64, kernel_size=2, activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=2, activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(filters=128, kernel_size=2, activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "#Compile\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train_c, y_train_c, validation_split=0.2, epochs=50)\n",
        "\n",
        "#Model Evaluation\n",
        "_, acc = model.evaluate(x_test_c, y_test_c, verbose=0)\n",
        "print('Accuracy: %.3f' % (acc*100))\n",
        "\n",
        "#Model Acc\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#Model Plot\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "comparison_df2 = pd.DataFrame(columns=['configuration','accuracy'])\n",
        "row={'configuration': 'Shallow ConvNet', 'accuracy': acc*100}\n",
        "comparison_df2=comparison_df2.append(row,ignore_index=True)\n",
        "\n",
        "row={'configuration': 'Shallow ConvNet', 'accuracy': acc*100}\n",
        "final_comparison_df=final_comparison_df.append(row,ignore_index=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.7630 - accuracy: 0.3616 - val_loss: 1.4968 - val_accuracy: 0.4689\n",
            "Epoch 2/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3923 - accuracy: 0.4985 - val_loss: 1.3351 - val_accuracy: 0.5224\n",
            "Epoch 3/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2216 - accuracy: 0.5655 - val_loss: 1.1341 - val_accuracy: 0.6032\n",
            "Epoch 4/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.0933 - accuracy: 0.6141 - val_loss: 1.0989 - val_accuracy: 0.6143\n",
            "Epoch 5/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9972 - accuracy: 0.6508 - val_loss: 0.9987 - val_accuracy: 0.6485\n",
            "Epoch 6/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9118 - accuracy: 0.6801 - val_loss: 0.9829 - val_accuracy: 0.6581\n",
            "Epoch 7/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8422 - accuracy: 0.7060 - val_loss: 0.9533 - val_accuracy: 0.6681\n",
            "Epoch 8/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7777 - accuracy: 0.7286 - val_loss: 0.8751 - val_accuracy: 0.6997\n",
            "Epoch 9/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7221 - accuracy: 0.7466 - val_loss: 0.9067 - val_accuracy: 0.6870\n",
            "Epoch 10/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6665 - accuracy: 0.7660 - val_loss: 0.8489 - val_accuracy: 0.7106\n",
            "Epoch 11/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6126 - accuracy: 0.7852 - val_loss: 0.8357 - val_accuracy: 0.7171\n",
            "Epoch 12/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.5672 - accuracy: 0.8023 - val_loss: 0.9390 - val_accuracy: 0.6935\n",
            "Epoch 13/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.5271 - accuracy: 0.8161 - val_loss: 0.8665 - val_accuracy: 0.7205\n",
            "Epoch 14/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.4742 - accuracy: 0.8346 - val_loss: 0.9117 - val_accuracy: 0.7182\n",
            "Epoch 15/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.4368 - accuracy: 0.8486 - val_loss: 0.9037 - val_accuracy: 0.7204\n",
            "Epoch 16/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.3954 - accuracy: 0.8616 - val_loss: 0.9231 - val_accuracy: 0.7224\n",
            "Epoch 17/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3542 - accuracy: 0.8758 - val_loss: 0.9142 - val_accuracy: 0.7311\n",
            "Epoch 18/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3168 - accuracy: 0.8868 - val_loss: 0.9677 - val_accuracy: 0.7255\n",
            "Epoch 19/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2794 - accuracy: 0.9017 - val_loss: 1.0790 - val_accuracy: 0.7174\n",
            "Epoch 20/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2446 - accuracy: 0.9132 - val_loss: 1.0989 - val_accuracy: 0.7202\n",
            "Epoch 21/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2139 - accuracy: 0.9240 - val_loss: 1.1237 - val_accuracy: 0.7210\n",
            "Epoch 22/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.1917 - accuracy: 0.9316 - val_loss: 1.2277 - val_accuracy: 0.7169\n",
            "Epoch 23/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1657 - accuracy: 0.9401 - val_loss: 1.3569 - val_accuracy: 0.7032\n",
            "Epoch 24/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1424 - accuracy: 0.9495 - val_loss: 1.3337 - val_accuracy: 0.7168\n",
            "Epoch 25/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.1389 - accuracy: 0.9518 - val_loss: 1.3545 - val_accuracy: 0.7130\n",
            "Epoch 26/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1114 - accuracy: 0.9608 - val_loss: 1.3771 - val_accuracy: 0.7207\n",
            "Epoch 27/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.1135 - accuracy: 0.9590 - val_loss: 1.4776 - val_accuracy: 0.7107\n",
            "Epoch 28/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0881 - accuracy: 0.9701 - val_loss: 1.5189 - val_accuracy: 0.7203\n",
            "Epoch 29/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0765 - accuracy: 0.9728 - val_loss: 1.5721 - val_accuracy: 0.7265\n",
            "Epoch 30/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0764 - accuracy: 0.9736 - val_loss: 1.7157 - val_accuracy: 0.7073\n",
            "Epoch 31/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0879 - accuracy: 0.9688 - val_loss: 1.6869 - val_accuracy: 0.7171\n",
            "Epoch 32/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0801 - accuracy: 0.9721 - val_loss: 1.6825 - val_accuracy: 0.7304\n",
            "Epoch 33/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0512 - accuracy: 0.9831 - val_loss: 1.7873 - val_accuracy: 0.7225\n",
            "Epoch 34/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0666 - accuracy: 0.9764 - val_loss: 1.7551 - val_accuracy: 0.7172\n",
            "Epoch 35/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0544 - accuracy: 0.9811 - val_loss: 1.8553 - val_accuracy: 0.7236\n",
            "Epoch 36/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0497 - accuracy: 0.9824 - val_loss: 1.7900 - val_accuracy: 0.7267\n",
            "Epoch 37/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0545 - accuracy: 0.9817 - val_loss: 1.8635 - val_accuracy: 0.7171\n",
            "Epoch 38/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0430 - accuracy: 0.9851 - val_loss: 1.9113 - val_accuracy: 0.7268\n",
            "Epoch 39/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0478 - accuracy: 0.9829 - val_loss: 1.9726 - val_accuracy: 0.7184\n",
            "Epoch 40/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0591 - accuracy: 0.9800 - val_loss: 1.9160 - val_accuracy: 0.7186\n",
            "Epoch 41/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0228 - accuracy: 0.9929 - val_loss: 1.9819 - val_accuracy: 0.7311\n",
            "Epoch 42/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0370 - accuracy: 0.9870 - val_loss: 1.9806 - val_accuracy: 0.7253\n",
            "Epoch 43/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0267 - accuracy: 0.9914 - val_loss: 2.0613 - val_accuracy: 0.7267\n",
            "Epoch 44/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0141 - accuracy: 0.9959 - val_loss: 2.0990 - val_accuracy: 0.7335\n",
            "Epoch 45/50\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.0117 - accuracy: 0.9964 - val_loss: 2.2667 - val_accuracy: 0.7253\n",
            "Epoch 46/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0295 - accuracy: 0.9899 - val_loss: 2.0924 - val_accuracy: 0.7226\n",
            "Epoch 47/50\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0353 - accuracy: 0.9875 - val_loss: 2.4044 - val_accuracy: 0.7050\n",
            "Epoch 48/50\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0424 - accuracy: 0.9852 - val_loss: 2.1267 - val_accuracy: 0.7182\n",
            "Epoch 49/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0307 - accuracy: 0.9898 - val_loss: 2.3242 - val_accuracy: 0.7201\n",
            "Epoch 50/50\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0331 - accuracy: 0.9887 - val_loss: 2.2740 - val_accuracy: 0.7241\n",
            "Accuracy: 70.980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NTjVIUkmv7S"
      },
      "source": [
        "#### Task 2.1.3 Train a ResNet\n",
        "\n",
        "Train a residual neural network (ResNet) on the CIFAR10 training data and report the test accuracy and the training time.\n",
        "\n",
        "The ResNet is a popular network architecture for image classification. You may find more information about how ResNet works by reading this [paper](https://arxiv.org/abs/1512.03385).\n",
        "\n",
        "\n",
        "*(You may implement a resnet model or use an existing implementation. In either case, you should not use pretrained network weights.)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106_DpxxzT94",
        "outputId": "5f13578c-33b4-438f-ee04-d692de1658c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Conv2D, BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.layers import add\n",
        "from keras.layers import Activation\n",
        "from keras.models import Model\n",
        "from keras import applications\n",
        "\n",
        "base_model = applications.resnet50.ResNet50(weights= None, include_top=False, input_shape= (32,32,3))\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.7)(x)\n",
        "layer_output = Dense(10, activation= 'softmax')(x)\n",
        "model = Model(inputs = base_model.input, outputs = layer_output)\n",
        "\n",
        "model.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_c, y_train_c, epochs = 50, batch_size = 64, validation_split=0.2)\n",
        "\n",
        "#Model Evaluation\n",
        "_, acc = model.evaluate(x_test_c, y_test_c, verbose=0)\n",
        "print('Accuracy: %.3f' % (acc*100))\n",
        "\n",
        "row={'configuration': 'ResNet', 'accuracy': acc*100}\n",
        "comparison_df2=comparison_df2.append(row,ignore_index=True)\n",
        "\n",
        "row={'configuration': 'ResNet', 'accuracy': acc*100}\n",
        "final_comparison_df=final_comparison_df.append(row,ignore_index=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "625/625 [==============================] - 27s 43ms/step - loss: 2.7553 - accuracy: 0.2700 - val_loss: 349.6628 - val_accuracy: 0.1017\n",
            "Epoch 2/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 2.7270 - accuracy: 0.2219 - val_loss: 6.0092 - val_accuracy: 0.2493\n",
            "Epoch 3/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 2.3637 - accuracy: 0.3068 - val_loss: 2.1110 - val_accuracy: 0.2125\n",
            "Epoch 4/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 2.1737 - accuracy: 0.3230 - val_loss: 2.0469 - val_accuracy: 0.3455\n",
            "Epoch 5/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 2.0641 - accuracy: 0.3858 - val_loss: 19.3764 - val_accuracy: 0.2603\n",
            "Epoch 6/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.9847 - accuracy: 0.4010 - val_loss: 2.1783 - val_accuracy: 0.3259\n",
            "Epoch 7/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.8661 - accuracy: 0.4336 - val_loss: 3.6815 - val_accuracy: 0.3830\n",
            "Epoch 8/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 2.2509 - accuracy: 0.3733 - val_loss: 1.8639 - val_accuracy: 0.3757\n",
            "Epoch 9/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 2.1133 - accuracy: 0.4072 - val_loss: 2.0818 - val_accuracy: 0.4116\n",
            "Epoch 10/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.9356 - accuracy: 0.4431 - val_loss: 2.1075 - val_accuracy: 0.4163\n",
            "Epoch 11/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.8990 - accuracy: 0.4606 - val_loss: 29.6583 - val_accuracy: 0.3407\n",
            "Epoch 12/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.9374 - accuracy: 0.4482 - val_loss: 39.8480 - val_accuracy: 0.3181\n",
            "Epoch 13/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.8342 - accuracy: 0.4629 - val_loss: 6.2437 - val_accuracy: 0.4391\n",
            "Epoch 14/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.8831 - accuracy: 0.4525 - val_loss: 2.0843 - val_accuracy: 0.3605\n",
            "Epoch 15/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.7229 - accuracy: 0.4889 - val_loss: 2.4599 - val_accuracy: 0.1691\n",
            "Epoch 16/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.6949 - accuracy: 0.4836 - val_loss: 6.4509 - val_accuracy: 0.2924\n",
            "Epoch 17/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.6868 - accuracy: 0.5024 - val_loss: 1.4282 - val_accuracy: 0.5106\n",
            "Epoch 18/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.5635 - accuracy: 0.5397 - val_loss: 1.6716 - val_accuracy: 0.3972\n",
            "Epoch 19/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.4390 - accuracy: 0.5550 - val_loss: 244.5898 - val_accuracy: 0.2560\n",
            "Epoch 20/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.3892 - accuracy: 0.5742 - val_loss: 1.7333 - val_accuracy: 0.4456\n",
            "Epoch 21/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.2995 - accuracy: 0.5925 - val_loss: 1.3175 - val_accuracy: 0.5624\n",
            "Epoch 22/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.3898 - accuracy: 0.5693 - val_loss: 5.4387 - val_accuracy: 0.4959\n",
            "Epoch 23/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.2812 - accuracy: 0.6060 - val_loss: 193.3368 - val_accuracy: 0.1473\n",
            "Epoch 24/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.5209 - accuracy: 0.5071 - val_loss: 2.5338 - val_accuracy: 0.3105\n",
            "Epoch 25/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.3774 - accuracy: 0.5479 - val_loss: 1.3649 - val_accuracy: 0.5843\n",
            "Epoch 26/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.1617 - accuracy: 0.6233 - val_loss: 2.2878 - val_accuracy: 0.3060\n",
            "Epoch 27/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.2900 - accuracy: 0.5700 - val_loss: 1.3470 - val_accuracy: 0.5444\n",
            "Epoch 28/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.1533 - accuracy: 0.6139 - val_loss: 1.2446 - val_accuracy: 0.5755\n",
            "Epoch 29/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.0979 - accuracy: 0.6292 - val_loss: 1.7579 - val_accuracy: 0.4039\n",
            "Epoch 30/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.0516 - accuracy: 0.6444 - val_loss: 1.1126 - val_accuracy: 0.6111\n",
            "Epoch 31/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.0455 - accuracy: 0.6415 - val_loss: 2.2751 - val_accuracy: 0.2999\n",
            "Epoch 32/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.1671 - accuracy: 0.5998 - val_loss: 1.1480 - val_accuracy: 0.6194\n",
            "Epoch 33/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.8953 - accuracy: 0.6902 - val_loss: 1.1508 - val_accuracy: 0.6166\n",
            "Epoch 34/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.9252 - accuracy: 0.6848 - val_loss: 1.0234 - val_accuracy: 0.6590\n",
            "Epoch 35/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.8425 - accuracy: 0.7162 - val_loss: 1.0447 - val_accuracy: 0.6583\n",
            "Epoch 36/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.0612 - accuracy: 0.6381 - val_loss: 1.2583 - val_accuracy: 0.5939\n",
            "Epoch 37/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.9192 - accuracy: 0.6882 - val_loss: 1.1464 - val_accuracy: 0.6353\n",
            "Epoch 38/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.8002 - accuracy: 0.7261 - val_loss: 1.3879 - val_accuracy: 0.6105\n",
            "Epoch 39/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.7464 - accuracy: 0.7412 - val_loss: 1.1417 - val_accuracy: 0.6749\n",
            "Epoch 40/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.9205 - accuracy: 0.6833 - val_loss: 1.1421 - val_accuracy: 0.6309\n",
            "Epoch 41/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.0739 - accuracy: 0.6278 - val_loss: 1.6424 - val_accuracy: 0.4602\n",
            "Epoch 42/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.0148 - accuracy: 0.6461 - val_loss: 1.4337 - val_accuracy: 0.6517\n",
            "Epoch 43/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.7605 - accuracy: 0.7351 - val_loss: 1.1567 - val_accuracy: 0.6895\n",
            "Epoch 44/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.6375 - accuracy: 0.7783 - val_loss: 2.6452 - val_accuracy: 0.6898\n",
            "Epoch 45/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.5954 - accuracy: 0.7940 - val_loss: 1.1800 - val_accuracy: 0.6719\n",
            "Epoch 46/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.5243 - accuracy: 0.8206 - val_loss: 2.1845 - val_accuracy: 0.6925\n",
            "Epoch 47/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.5530 - accuracy: 0.8098 - val_loss: 3.8326 - val_accuracy: 0.3588\n",
            "Epoch 48/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.5459 - accuracy: 0.8106 - val_loss: 1.9642 - val_accuracy: 0.6721\n",
            "Epoch 49/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.5468 - accuracy: 0.8116 - val_loss: 1.0355 - val_accuracy: 0.6619\n",
            "Epoch 50/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.4562 - accuracy: 0.8429 - val_loss: 2.1358 - val_accuracy: 0.6701\n",
            "Accuracy: 67.320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH6ZBiECzS75"
      },
      "source": [
        "### Task 2.2 Fast training of ResNet\n",
        "\n",
        "*(weight ~5%)*\n",
        "\n",
        "In this task, you will experiment with different ways to reduce the time for training your ResNet on CIFAR10. There are different ways to speed up neural network training; below are two ideas. Please select at least one idea to implement. Explain the experiment steps and report the final performance and training time.\n",
        "\n",
        "#### Option 1. Learning rate schedule\n",
        "\n",
        "Use a learning rate schedule for the training. Some popular learning rate schedules include \n",
        "\n",
        "- the Step Decay learning rate (e.g., see [here](https://github.com/kuangliu/pytorch-cifar))\n",
        "- [Cyclical learning rates](https://arxiv.org/abs/1506.01186)\n",
        "- [The exponential learning rate](https://openreview.net/forum?id=rJg8TeSFDH) \n",
        "\n",
        "Also Keras provides [some convenient functions](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules) that you can use.\n",
        "\n",
        "\n",
        "#### Option 2. Look ahead optimiser\n",
        "\n",
        "Read [this paper](https://arxiv.org/abs/1907.08610) and implement the Lookahead optimiser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKDB2ZdZ4eNq",
        "outputId": "d6b67c16-fe99-4d92-aa86-0184e9298d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "initial_learning_rate = 0.1\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=100000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "# decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
        "\n",
        "model = applications.resnet50.ResNet50(weights= None, include_top=False, input_shape= (32,32,3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.7)(x)\n",
        "layer_output = Dense(10, activation= 'softmax')(x)\n",
        "model = Model(inputs = base_model.input, outputs = layer_output)\n",
        "\n",
        "model.compile(optimizer=SGD(learning_rate=lr_schedule, momentum=0.9),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_c, y_train_c, epochs = 50, batch_size=64, validation_split=0.2)\n",
        "\n",
        "#Model Evaluation\n",
        "_, acc = model.evaluate(x_test_c, y_test_c, verbose=0)\n",
        "print('Accuracy: %.3f' % (acc*100))\n",
        "\n",
        "row={'configuration': 'ResNet with Exponential LR', 'accuracy': score[1]*100}\n",
        "comparison_df2=comparison_df2.append(row,ignore_index=True)\n",
        "\n",
        "row={'configuration': 'ResNet with Exponential LR', 'accuracy': acc*100}\n",
        "final_comparison_df=final_comparison_df.append(row,ignore_index=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "625/625 [==============================] - 27s 43ms/step - loss: 1.1016 - accuracy: 0.6312 - val_loss: 69.3114 - val_accuracy: 0.3311\n",
            "Epoch 2/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.3747 - accuracy: 0.5095 - val_loss: 3.0207 - val_accuracy: 0.3452\n",
            "Epoch 3/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.5706 - accuracy: 0.4404 - val_loss: 1.9257 - val_accuracy: 0.4789\n",
            "Epoch 4/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.3850 - accuracy: 0.5170 - val_loss: 1.5061 - val_accuracy: 0.5041\n",
            "Epoch 5/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.3539 - accuracy: 0.5304 - val_loss: 30.5717 - val_accuracy: 0.3751\n",
            "Epoch 6/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.2242 - accuracy: 0.5787 - val_loss: 1.3070 - val_accuracy: 0.5775\n",
            "Epoch 7/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.0257 - accuracy: 0.6484 - val_loss: 1.1007 - val_accuracy: 0.6241\n",
            "Epoch 8/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.5861 - accuracy: 0.4339 - val_loss: 27.0859 - val_accuracy: 0.3490\n",
            "Epoch 9/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.8399 - accuracy: 0.3311 - val_loss: 2.0457 - val_accuracy: 0.2611\n",
            "Epoch 10/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.9764 - accuracy: 0.2783 - val_loss: 1.9552 - val_accuracy: 0.3612\n",
            "Epoch 11/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.8464 - accuracy: 0.3321 - val_loss: 2.7239 - val_accuracy: 0.3714\n",
            "Epoch 12/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.6657 - accuracy: 0.4016 - val_loss: 2.5633 - val_accuracy: 0.4230\n",
            "Epoch 13/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.6158 - accuracy: 0.4178 - val_loss: 2.2829 - val_accuracy: 0.4548\n",
            "Epoch 14/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.5416 - accuracy: 0.4455 - val_loss: 2.0691 - val_accuracy: 0.4649\n",
            "Epoch 15/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.4768 - accuracy: 0.4743 - val_loss: 2.2805 - val_accuracy: 0.4827\n",
            "Epoch 16/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.5157 - accuracy: 0.4592 - val_loss: 4748.5850 - val_accuracy: 0.0880\n",
            "Epoch 17/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.6454 - accuracy: 0.4139 - val_loss: 2.4192 - val_accuracy: 0.4490\n",
            "Epoch 18/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.7975 - accuracy: 0.3473 - val_loss: 2.5531 - val_accuracy: 0.2145\n",
            "Epoch 19/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.8274 - accuracy: 0.3319 - val_loss: 45.0221 - val_accuracy: 0.3020\n",
            "Epoch 20/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.6932 - accuracy: 0.3899 - val_loss: 2.3165 - val_accuracy: 0.4319\n",
            "Epoch 21/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.5986 - accuracy: 0.4272 - val_loss: 3.7390 - val_accuracy: 0.4620\n",
            "Epoch 22/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.7885 - accuracy: 0.3510 - val_loss: 204.6634 - val_accuracy: 0.1075\n",
            "Epoch 23/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 2.0119 - accuracy: 0.2461 - val_loss: 15.2459 - val_accuracy: 0.3032\n",
            "Epoch 24/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 2.2712 - accuracy: 0.1294 - val_loss: 2.3334 - val_accuracy: 0.1342\n",
            "Epoch 25/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 2.2324 - accuracy: 0.1542 - val_loss: 2.5832 - val_accuracy: 0.1707\n",
            "Epoch 26/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 2.1568 - accuracy: 0.1887 - val_loss: 2.0702 - val_accuracy: 0.2228\n",
            "Epoch 27/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 2.0523 - accuracy: 0.2254 - val_loss: 1.9772 - val_accuracy: 0.2514\n",
            "Epoch 28/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.9850 - accuracy: 0.2463 - val_loss: 2.0026 - val_accuracy: 0.2534\n",
            "Epoch 29/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.9475 - accuracy: 0.2616 - val_loss: 1.9898 - val_accuracy: 0.2790\n",
            "Epoch 30/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.9219 - accuracy: 0.2772 - val_loss: 2.1354 - val_accuracy: 0.2643\n",
            "Epoch 31/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.8702 - accuracy: 0.2978 - val_loss: 1.9814 - val_accuracy: 0.3211\n",
            "Epoch 32/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.8414 - accuracy: 0.3154 - val_loss: 3.2946 - val_accuracy: 0.2332\n",
            "Epoch 33/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.8472 - accuracy: 0.3171 - val_loss: 2.0497 - val_accuracy: 0.3498\n",
            "Epoch 34/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.8204 - accuracy: 0.3228 - val_loss: 1.8893 - val_accuracy: 0.3564\n",
            "Epoch 35/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.7948 - accuracy: 0.3366 - val_loss: 2.2079 - val_accuracy: 0.3479\n",
            "Epoch 36/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.7572 - accuracy: 0.3513 - val_loss: 1.9390 - val_accuracy: 0.3801\n",
            "Epoch 37/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.7390 - accuracy: 0.3581 - val_loss: 2.1875 - val_accuracy: 0.2242\n",
            "Epoch 38/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.8271 - accuracy: 0.3251 - val_loss: 252.8338 - val_accuracy: 0.1125\n",
            "Epoch 39/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.9015 - accuracy: 0.3036 - val_loss: 2.8603 - val_accuracy: 0.3081\n",
            "Epoch 40/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.8948 - accuracy: 0.3048 - val_loss: 2.0183 - val_accuracy: 0.3469\n",
            "Epoch 41/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.8030 - accuracy: 0.3407 - val_loss: 1.8680 - val_accuracy: 0.3686\n",
            "Epoch 42/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.7618 - accuracy: 0.3566 - val_loss: 1.7629 - val_accuracy: 0.3822\n",
            "Epoch 43/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.7331 - accuracy: 0.3689 - val_loss: 1.6727 - val_accuracy: 0.3777\n",
            "Epoch 44/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.7217 - accuracy: 0.3715 - val_loss: 2.3827 - val_accuracy: 0.3343\n",
            "Epoch 45/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.7265 - accuracy: 0.3723 - val_loss: 1.7749 - val_accuracy: 0.3521\n",
            "Epoch 46/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.7363 - accuracy: 0.3704 - val_loss: 1.9142 - val_accuracy: 0.3347\n",
            "Epoch 47/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.7430 - accuracy: 0.3663 - val_loss: 1.7202 - val_accuracy: 0.3790\n",
            "Epoch 48/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.7001 - accuracy: 0.3787 - val_loss: 49.0406 - val_accuracy: 0.3261\n",
            "Epoch 49/50\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 1.7202 - accuracy: 0.3767 - val_loss: 1.7131 - val_accuracy: 0.4082\n",
            "Epoch 50/50\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 1.7255 - accuracy: 0.3704 - val_loss: 2.0031 - val_accuracy: 0.3806\n",
            "Accuracy: 38.420\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8cKfAOjpn7c"
      },
      "source": [
        "### Task 2.3 Performance comparison\n",
        "\n",
        "*(weight ~5%)*\n",
        "\n",
        "Based on the above experiments, which method or which combination of methods result in the best accuracy with the same training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Q1LlzB4hGR",
        "outputId": "74a1637b-9648-49c8-9452-7ddea70924c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "comparison_df2"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>configuration</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Shallow ConvNet</td>\n",
              "      <td>70.980000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ResNet</td>\n",
              "      <td>67.320001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ResNet with Exponential LR</td>\n",
              "      <td>92.369998</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                configuration   accuracy\n",
              "0             Shallow ConvNet  70.980000\n",
              "1                      ResNet  67.320001\n",
              "2  ResNet with Exponential LR  92.369998"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3RXruOF4i2k",
        "outputId": "d4f969a2-6da3-485c-e629-4a31292d59c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "highest=comparison_df2['accuracy'].max()\n",
        "print(\"The highest accuracy is observed for:\\n\")\n",
        "comparison_df2[comparison_df2['accuracy']==highest]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The highest accuracy is observed for:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>configuration</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ResNet with Exponential LR</td>\n",
              "      <td>92.369998</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                configuration   accuracy\n",
              "2  ResNet with Exponential LR  92.369998"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x73J5Qkqjnfu"
      },
      "source": [
        "## Task 3 Design a novel deep neural network model (Challenge Task for Targeting HD Grades)\n",
        "\n",
        "*(weight ~11%)*\n",
        "Here, you have to show your critical idea to design a new neural network model. We will evaluate your results based on the novelty of the model and performance of the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvJ7KXImjnfv"
      },
      "source": [
        "### Task 3.1: The key idea to design a novel deep neural networks for CIFAR10\n",
        "\n",
        "*(weight ~5%)*\n",
        "\n",
        "In this task, you will design a novel deep neural networks on the [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). CIFAR10 represents a relatively larger multi-class classification problem and presents a great opportunity for you to solve a \"harder\" problem. Different from Task 2, in this task you are required to design a novel neural network and optimize the performance in classification. In your answer, you have to clearly present what the key difference between your model and the classic ones, what the benefits in your design model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq9pAPNO4lkK",
        "outputId": "a7fc90eb-2b80-4062-c4a1-7fe66e99dba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#AUTOENCODER-RNN\n",
        "\n",
        "#!pip install tensorflow-gpu\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation\n",
        "\n",
        "(x_train_c, y_train_c), (x_test_c, y_test_c) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "y_train_c = tf.keras.utils.to_categorical(y_train_c)\n",
        "y_test_c = tf.keras.utils.to_categorical(y_test_c)\n",
        "\n",
        "x_train_re = keras.utils.normalize(x_train_c)\n",
        "x_test_re  = keras.utils.normalize(x_test_c)\n",
        "\n",
        "def CONVautoencoder(x_train_re,x_test_re,epochs=50):\n",
        "    ## functional approach\n",
        "    ## input dimension is 3 so the output dimension will be 3\n",
        "    \n",
        "    input_img = Input(shape=(32, 32, 3))\n",
        "    x = Conv2D(64, (3, 3), padding='same')(input_img)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(32, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(16, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    x = Conv2D(16, (3, 3), padding='same')(encoded)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(32, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(3, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    decoded = Activation('sigmoid')(x)\n",
        "    autoencoder = Model(input_img,decoded)\n",
        "    encoder = Model(input_img,encoded)\n",
        "    autoencoder.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "    ## we dont need to compile the encoder model\n",
        "    ## it is the sub model of the auto encoder\n",
        "\n",
        "    print(autoencoder.summary())\n",
        "    print(encoder.summary())\n",
        "    autoencoder.fit(x_train_re,x_train_re,epochs=50,batch_size=64,validation_data=(x_test_re,x_test_re))\n",
        "    \n",
        "    encoded_imgs=encoder.predict(x_test_re)\n",
        "    predicted = autoencoder.predict(x_test_re)\n",
        "    return encoded_imgs,predicted\n",
        "\n",
        "    return encoded_imgs,predicted\n",
        "\n",
        "encoded_imgs,predicted = CONVautoencoder(x_train_re,x_test_re)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 16, 16, 32)        18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 16, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 8, 8, 16)          4624      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 8, 8, 32)          4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 8, 8, 32)          128       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 32, 32, 3)         1731      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 32, 32, 3)         12        \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 32, 32, 3)         0         \n",
            "=================================================================\n",
            "Total params: 52,975\n",
            "Trainable params: 52,521\n",
            "Non-trainable params: 454\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"functional_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 16, 16, 32)        18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 16, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 8, 8, 16)          4624      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 4, 4, 16)          0         \n",
            "=================================================================\n",
            "Total params: 25,328\n",
            "Trainable params: 25,104\n",
            "Non-trainable params: 224\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.6740 - accuracy: 0.6973 - val_loss: 0.6638 - val_accuracy: 0.7772\n",
            "Epoch 2/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6623 - accuracy: 0.7702 - val_loss: 0.6615 - val_accuracy: 0.7879\n",
            "Epoch 3/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6612 - accuracy: 0.7860 - val_loss: 0.6609 - val_accuracy: 0.8052\n",
            "Epoch 4/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6607 - accuracy: 0.7928 - val_loss: 0.6603 - val_accuracy: 0.8165\n",
            "Epoch 5/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6603 - accuracy: 0.7978 - val_loss: 0.6601 - val_accuracy: 0.7979\n",
            "Epoch 6/50\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.6601 - accuracy: 0.8020 - val_loss: 0.6600 - val_accuracy: 0.8102\n",
            "Epoch 7/50\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.6599 - accuracy: 0.8038 - val_loss: 0.6598 - val_accuracy: 0.8167\n",
            "Epoch 8/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6597 - accuracy: 0.8071 - val_loss: 0.6596 - val_accuracy: 0.8248\n",
            "Epoch 9/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6596 - accuracy: 0.8083 - val_loss: 0.6595 - val_accuracy: 0.8067\n",
            "Epoch 10/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6595 - accuracy: 0.8091 - val_loss: 0.6598 - val_accuracy: 0.8093\n",
            "Epoch 11/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6594 - accuracy: 0.8095 - val_loss: 0.6593 - val_accuracy: 0.8355\n",
            "Epoch 12/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6593 - accuracy: 0.8114 - val_loss: 0.6596 - val_accuracy: 0.8095\n",
            "Epoch 13/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6592 - accuracy: 0.8118 - val_loss: 0.6591 - val_accuracy: 0.8212\n",
            "Epoch 14/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6592 - accuracy: 0.8123 - val_loss: 0.6594 - val_accuracy: 0.8038\n",
            "Epoch 15/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6592 - accuracy: 0.8133 - val_loss: 0.6592 - val_accuracy: 0.7827\n",
            "Epoch 16/50\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.6591 - accuracy: 0.8141 - val_loss: 0.6589 - val_accuracy: 0.8299\n",
            "Epoch 17/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6591 - accuracy: 0.8139 - val_loss: 0.6591 - val_accuracy: 0.7998\n",
            "Epoch 18/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6590 - accuracy: 0.8148 - val_loss: 0.6589 - val_accuracy: 0.8135\n",
            "Epoch 19/50\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.6590 - accuracy: 0.8139 - val_loss: 0.6604 - val_accuracy: 0.7712\n",
            "Epoch 20/50\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.6589 - accuracy: 0.8150 - val_loss: 0.6591 - val_accuracy: 0.8358\n",
            "Epoch 21/50\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.6589 - accuracy: 0.8163 - val_loss: 0.6594 - val_accuracy: 0.7450\n",
            "Epoch 22/50\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.6589 - accuracy: 0.8150 - val_loss: 0.6591 - val_accuracy: 0.8259\n",
            "Epoch 23/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6588 - accuracy: 0.8167 - val_loss: 0.6588 - val_accuracy: 0.8209\n",
            "Epoch 24/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6589 - accuracy: 0.8149 - val_loss: 0.6587 - val_accuracy: 0.8121\n",
            "Epoch 25/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6588 - accuracy: 0.8174 - val_loss: 0.6586 - val_accuracy: 0.8430\n",
            "Epoch 26/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6588 - accuracy: 0.8178 - val_loss: 0.6588 - val_accuracy: 0.8070\n",
            "Epoch 27/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6588 - accuracy: 0.8168 - val_loss: 0.6589 - val_accuracy: 0.8114\n",
            "Epoch 28/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6588 - accuracy: 0.8195 - val_loss: 0.6586 - val_accuracy: 0.8314\n",
            "Epoch 29/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6588 - accuracy: 0.8175 - val_loss: 0.6587 - val_accuracy: 0.8127\n",
            "Epoch 30/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6588 - accuracy: 0.8183 - val_loss: 0.6587 - val_accuracy: 0.8442\n",
            "Epoch 31/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6587 - accuracy: 0.8177 - val_loss: 0.6586 - val_accuracy: 0.8156\n",
            "Epoch 32/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6587 - accuracy: 0.8182 - val_loss: 0.6588 - val_accuracy: 0.8347\n",
            "Epoch 33/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6587 - accuracy: 0.8185 - val_loss: 0.6587 - val_accuracy: 0.8492\n",
            "Epoch 34/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6587 - accuracy: 0.8177 - val_loss: 0.6589 - val_accuracy: 0.7917\n",
            "Epoch 35/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6587 - accuracy: 0.8173 - val_loss: 0.6589 - val_accuracy: 0.8268\n",
            "Epoch 36/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6587 - accuracy: 0.8184 - val_loss: 0.6591 - val_accuracy: 0.7937\n",
            "Epoch 37/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6587 - accuracy: 0.8188 - val_loss: 0.6585 - val_accuracy: 0.8465\n",
            "Epoch 38/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6587 - accuracy: 0.8189 - val_loss: 0.6585 - val_accuracy: 0.8237\n",
            "Epoch 39/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6586 - accuracy: 0.8206 - val_loss: 0.6587 - val_accuracy: 0.8044\n",
            "Epoch 40/50\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.6587 - accuracy: 0.8188 - val_loss: 0.6586 - val_accuracy: 0.8296\n",
            "Epoch 41/50\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.6586 - accuracy: 0.8187 - val_loss: 0.6586 - val_accuracy: 0.8151\n",
            "Epoch 42/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6586 - accuracy: 0.8206 - val_loss: 0.6586 - val_accuracy: 0.8254\n",
            "Epoch 43/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6586 - accuracy: 0.8176 - val_loss: 0.6586 - val_accuracy: 0.8460\n",
            "Epoch 44/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6586 - accuracy: 0.8190 - val_loss: 0.6585 - val_accuracy: 0.8392\n",
            "Epoch 45/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6586 - accuracy: 0.8180 - val_loss: 0.6585 - val_accuracy: 0.8345\n",
            "Epoch 46/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6586 - accuracy: 0.8199 - val_loss: 0.6586 - val_accuracy: 0.8349\n",
            "Epoch 47/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6586 - accuracy: 0.8197 - val_loss: 0.6584 - val_accuracy: 0.8311\n",
            "Epoch 48/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6586 - accuracy: 0.8214 - val_loss: 0.6585 - val_accuracy: 0.8343\n",
            "Epoch 49/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6586 - accuracy: 0.8194 - val_loss: 0.6586 - val_accuracy: 0.8351\n",
            "Epoch 50/50\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6586 - accuracy: 0.8200 - val_loss: 0.6586 - val_accuracy: 0.8239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeI1WVpO4q_y",
        "outputId": "79dc6db7-8a7b-4e97-97e2-4dab4cdce98f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"The key difference between the autoencoder and classic model is that in rnn model output from the previous step is fed as input to the current step\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The key difference between the autoencoder and classic model is that in rnn model output from the previous step is fed as input to the current step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiABNBTqjnfw"
      },
      "source": [
        "### Task 3.2: The implementation of the novel deep neural networks for CIFAR10\n",
        "\n",
        "*(weight ~6%)*\n",
        "\n",
        "In this task, it requires you to write the codes for model implementation and report the performance. In your results, you have to demonstrate the compared performance of your new model and the state-of-the-art models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtWIvw1V4txJ",
        "outputId": "c4b62940-b4b8-4924-f59c-2279c4eb5443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "final_comparison_df"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>configuration</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ConvNet</td>\n",
              "      <td>88.929999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ImageDataGenerator</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Transfer Learning with Fine Tuning</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Transfer Learning without Fine Tuning</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Shallow ConvNet</td>\n",
              "      <td>70.980000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ResNet</td>\n",
              "      <td>67.320001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ResNet with Exponential LR</td>\n",
              "      <td>38.420001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           configuration   accuracy\n",
              "0                                ConvNet  88.929999\n",
              "1                     ImageDataGenerator  10.000000\n",
              "2     Transfer Learning with Fine Tuning  10.000000\n",
              "3  Transfer Learning without Fine Tuning  10.000000\n",
              "4                        Shallow ConvNet  70.980000\n",
              "5                                 ResNet  67.320001\n",
              "6             ResNet with Exponential LR  38.420001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUV0wuZ01DNA"
      },
      "source": [
        "---\n",
        "**END OF ASSIGNMENT TWO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KIzEsKdjqpV"
      },
      "source": [
        "**Html Output Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv9pXpiLV9Zh",
        "outputId": "ba8d2021-fba9-4fc9-a46b-4ce6c45e05be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZcXnaqajp-O",
        "outputId": "a2fe45b9-60e2-4e07-d8bc-f398f574e386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "!pip install nbconvert"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (5.6.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.8.4)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.6.1)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (5.0.7)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (1.4.2)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.11.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.4.4)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.3.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert) (3.2.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.6.3)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert) (1.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert) (4.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (20.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach->nbconvert) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ0eVsySj2DO",
        "outputId": "917cd169-9782-44b0-aacb-04c46643e46d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!jupyter nbconvert --to html Copy of 218612723_Assignment_2_solution.ipynb"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[NbConvertApp] WARNING | pattern u'Copy' matched no files\n",
            "[NbConvertApp] WARNING | pattern u'of' matched no files\n",
            "[NbConvertApp] WARNING | pattern u'218612723_Assignment_2_solution.ipynb' matched no files\n",
            "This application is used to convert notebook files (*.ipynb) to various other\n",
            "formats.\n",
            "\n",
            "WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
            "\n",
            "Options\n",
            "-------\n",
            "\n",
            "Arguments that take values are actually convenience aliases to full\n",
            "Configurables, whose aliases are listed on the help line. For more information\n",
            "on full configurables, see '--help-all'.\n",
            "\n",
            "--execute\n",
            "    Execute the notebook prior to export.\n",
            "--allow-errors\n",
            "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
            "--no-input\n",
            "    Exclude input cells and output prompts from converted document. \n",
            "    This mode is ideal for generating code-free reports.\n",
            "--stdout\n",
            "    Write notebook output to stdout instead of files.\n",
            "--stdin\n",
            "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
            "--inplace\n",
            "    Run nbconvert in place, overwriting the existing notebook (only \n",
            "    relevant when converting to notebook format)\n",
            "-y\n",
            "    Answer yes to any questions instead of prompting.\n",
            "--clear-output\n",
            "    Clear output of current file and save in place, \n",
            "    overwriting the existing notebook.\n",
            "--debug\n",
            "    set log level to logging.DEBUG (maximize logging output)\n",
            "--no-prompt\n",
            "    Exclude input and output prompts from converted document.\n",
            "--generate-config\n",
            "    generate default config file\n",
            "--nbformat=<Enum> (NotebookExporter.nbformat_version)\n",
            "    Default: 4\n",
            "    Choices: [1, 2, 3, 4]\n",
            "    The nbformat version to write. Use this to downgrade notebooks.\n",
            "--output-dir=<Unicode> (FilesWriter.build_directory)\n",
            "    Default: ''\n",
            "    Directory to write output(s) to. Defaults to output to the directory of each\n",
            "    notebook. To recover previous default behaviour (outputting to the current\n",
            "    working directory) use . as the flag value.\n",
            "--writer=<DottedObjectName> (NbConvertApp.writer_class)\n",
            "    Default: 'FilesWriter'\n",
            "    Writer class used to write the  results of the conversion\n",
            "--log-level=<Enum> (Application.log_level)\n",
            "    Default: 30\n",
            "    Choices: (0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL')\n",
            "    Set the log level by value or name.\n",
            "--reveal-prefix=<Unicode> (SlidesExporter.reveal_url_prefix)\n",
            "    Default: u''\n",
            "    The URL prefix for reveal.js (version 3.x). This defaults to the reveal CDN,\n",
            "    but can be any url pointing to a copy  of reveal.js.\n",
            "    For speaker notes to work, this must be a relative path to a local  copy of\n",
            "    reveal.js: e.g., \"reveal.js\".\n",
            "    If a relative path is given, it must be a subdirectory of the current\n",
            "    directory (from which the server is run).\n",
            "    See the usage documentation\n",
            "    (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-\n",
            "    slideshow) for more details.\n",
            "--to=<Unicode> (NbConvertApp.export_format)\n",
            "    Default: 'html'\n",
            "    The export format to be used, either one of the built-in formats\n",
            "    ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf',\n",
            "    'python', 'rst', 'script', 'slides'] or a dotted object name that represents\n",
            "    the import path for an `Exporter` class\n",
            "--template=<Unicode> (TemplateExporter.template_file)\n",
            "    Default: u''\n",
            "    Name of the template file to use\n",
            "--output=<Unicode> (NbConvertApp.output_base)\n",
            "    Default: ''\n",
            "    overwrite base name use for output files. can only be used when converting\n",
            "    one notebook at a time.\n",
            "--post=<DottedOrNone> (NbConvertApp.postprocessor_class)\n",
            "    Default: u''\n",
            "    PostProcessor class used to write the results of the conversion\n",
            "--config=<Unicode> (JupyterApp.config_file)\n",
            "    Default: u''\n",
            "    Full path of a config file.\n",
            "\n",
            "To see all available configurables, use `--help-all`\n",
            "\n",
            "Examples\n",
            "--------\n",
            "\n",
            "    The simplest way to use nbconvert is\n",
            "    \n",
            "    > jupyter nbconvert mynotebook.ipynb\n",
            "    \n",
            "    which will convert mynotebook.ipynb to the default format (probably HTML).\n",
            "    \n",
            "    You can specify the export format with `--to`.\n",
            "    Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides'].\n",
            "    \n",
            "    > jupyter nbconvert --to latex mynotebook.ipynb\n",
            "    \n",
            "    Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
            "    'base', 'article' and 'report'.  HTML includes 'basic' and 'full'. You\n",
            "    can specify the flavor of the format used.\n",
            "    \n",
            "    > jupyter nbconvert --to html --template basic mynotebook.ipynb\n",
            "    \n",
            "    You can also pipe the output to stdout, rather than a file\n",
            "    \n",
            "    > jupyter nbconvert mynotebook.ipynb --stdout\n",
            "    \n",
            "    PDF is generated via latex\n",
            "    \n",
            "    > jupyter nbconvert mynotebook.ipynb --to pdf\n",
            "    \n",
            "    You can get (and serve) a Reveal.js-powered slideshow\n",
            "    \n",
            "    > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
            "    \n",
            "    Multiple notebooks can be given at the command line in a couple of \n",
            "    different ways:\n",
            "    \n",
            "    > jupyter nbconvert notebook*.ipynb\n",
            "    > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
            "    \n",
            "    or you can specify the notebooks list in a config file, containing::\n",
            "    \n",
            "        c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
            "    \n",
            "    > jupyter nbconvert --config mycfg.py\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q1Bdt2pUUo2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaBCKr4_UYI_",
        "outputId": "66302b88-93a1-4222-f33d-d35854a594b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nwA7nNzUZC1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}